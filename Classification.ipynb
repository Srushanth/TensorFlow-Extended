{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tfx import v1 as tfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensorflow version = 2.9.1\n",
      "tfx version        = 1.9.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "tensorflow version = {tf.__version__}\n",
    "tfx version        = {tfx.__version__}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_NAME='penguin-simple'\n",
    "# Output directory to store artifacts generated from the pipeline.\n",
    "PIPELINE_ROOT=os.path.join('pipelines', PIPELINE_NAME)\n",
    "# Path to a SQLite DB file to use as an MLMD storage.\n",
    "METADATA_PATH=os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "SERVING_MODEL_DIR=os.path.join('serving_model', PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PIPELINE_NAME=penguin-simple\n",
      "PIPELINE_ROOT=pipelines\\penguin-simple\n",
      "METADATA_PATH=metadata\\penguin-simple\\metadata.db\n",
      "SERVING_MODEL_DIR=serving_model\\penguin-simple\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "PIPELINE_NAME={PIPELINE_NAME}\n",
    "PIPELINE_ROOT={PIPELINE_ROOT}\n",
    "METADATA_PATH={METADATA_PATH}\n",
    "SERVING_MODEL_DIR={SERVING_MODEL_DIR}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default logging level.\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Srush\\\\AppData\\\\Local\\\\Temp\\\\tfx-dataaj0w7pr4\\\\data.csv',\n",
       " <http.client.HTTPMessage at 0x1c4a7a37400>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a temperory directory\n",
    "DATA_ROOT=tempfile.mkdtemp(prefix='tfx-data')\n",
    "_data_url='https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv'\n",
    "_data_filepath=os.path.join(DATA_ROOT, 'data.csv')\n",
    "urllib.request.urlretrieve(_data_url, _data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "_trainer_module_file='penguin_trainer.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_pipeline(\n",
    "    pipeline_name: str, \n",
    "    pipeline_root: str, \n",
    "    data_root: str,\n",
    "    module_file: str, \n",
    "    serving_model_dir: str,\n",
    "    metadata_path: str) -> tfx.dsl.Pipeline:\n",
    "    '''Creates a three component penguin pipeline with TFX.'''\n",
    "    # Brings data into the pipeline.\n",
    "    example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "    # Uses user-provided Python function that trains a model.\n",
    "    trainer = tfx.components.Trainer(\n",
    "        module_file=module_file,\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        train_args=tfx.proto.TrainArgs(num_steps=100),\n",
    "        eval_args=tfx.proto.EvalArgs(num_steps=5)\n",
    "    )\n",
    "\n",
    "    # Pushes the model to a filesystem destination.\n",
    "    pusher = tfx.components.Pusher(\n",
    "        model=trainer.outputs['model'],\n",
    "        push_destination=tfx.proto.PushDestination(\n",
    "            filesystem=tfx.proto.PushDestination.Filesystem(\n",
    "                base_directory=serving_model_dir\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Following three components will be included in the pipeline.\n",
    "    components = [\n",
    "        example_gen,\n",
    "        trainer,\n",
    "        pusher,\n",
    "    ]\n",
    "    \n",
    "    return tfx.dsl.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        metadata_connection_config=tfx.orchestration.metadata.sqlite_metadata_connection_config(metadata_path),\n",
    "        components=components\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Generating ephemeral wheel package for 'c:\\\\Users\\\\Srush\\\\Documents\\\\GitHub\\\\TensorFlow-Extended\\\\penguin_trainer.py' (including modules: ['penguin_trainer']).\n",
      "INFO:absl:User module package has hash fingerprint version 31faafa906472d00344224fb077aab0b0502c36c6ce9301e65a92720ce2934ad.\n",
      "INFO:absl:Executing: ['c:\\\\ProgramData\\\\Anaconda3\\\\python.exe', 'C:\\\\Users\\\\Srush\\\\AppData\\\\Local\\\\Temp\\\\tmpvrur1xvx\\\\_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', 'C:\\\\Users\\\\Srush\\\\AppData\\\\Local\\\\Temp\\\\tmpycniyu0h', '--dist-dir', 'C:\\\\Users\\\\Srush\\\\AppData\\\\Local\\\\Temp\\\\tmpoxdjlkeg']\n",
      "INFO:absl:Successfully built user code wheel distribution at 'pipelines\\\\penguin-simple\\\\_wheels\\\\tfx_user_code_Trainer-0.0+31faafa906472d00344224fb077aab0b0502c36c6ce9301e65a92720ce2934ad-py3-none-any.whl'; target user module is 'penguin_trainer'.\n",
      "INFO:absl:Full user module path is 'penguin_trainer@pipelines\\\\penguin-simple\\\\_wheels\\\\tfx_user_code_Trainer-0.0+31faafa906472d00344224fb077aab0b0502c36c6ce9301e65a92720ce2934ad-py3-none-any.whl'\n",
      "INFO:absl:Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Pusher\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.pusher.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Trainer\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  database_connection_config {\n",
      "    sqlite {\n",
      "      filename_uri: \"metadata\\\\penguin-simple\\\\metadata.db\"\n",
      "      connection_mode: READWRITE_OPENCREATE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \"metadata\\\\penguin-simple\\\\metadata.db\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "INFO:absl:Component CsvExampleGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-simple\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-08-27T18:32:54.408901\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-simple.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"C:\\\\Users\\\\Srush\\\\AppData\\\\Local\\\\Temp\\\\tfx-dataaj0w7pr4\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:select span and version = (0, None)\n",
      "INFO:absl:latest span and version = (0, None)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 4\n",
      "ERROR:absl:Failed to make stateful working dir: pipelines\\penguin-simple\\CsvExampleGen\\.system\\stateful_working_dir\\2022-08-27T18:32:54.408901\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\tfx\\orchestration\\portable\\outputs_utils.py\", line 223, in get_stateful_working_directory\n",
      "    fileio.makedirs(stateful_working_dir)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\tfx\\dsl\\io\\fileio.py\", line 80, in makedirs\n",
      "    _get_filesystem(path).makedirs(path)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\tfx\\dsl\\io\\plugins\\tensorflow_gfile.py\", line 76, in makedirs\n",
      "    tf.io.gfile.makedirs(path)\n",
      "  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 511, in recursive_create_dir_v2\n",
      "    _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Failed to create a directory: pipelines\\penguin-simple\\CsvExampleGen\\.system\\stateful_working_dir/2022-08-27T18:32:54.408901; Invalid argument\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Failed to create a directory: pipelines\\penguin-simple\\CsvExampleGen\\.system\\stateful_working_dir/2022-08-27T18:32:54.408901; Invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20620/3460669014.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m tfx.orchestration.LocalDagRunner().run(\n\u001b[0m\u001b[0;32m      2\u001b[0m     _create_pipeline(\n\u001b[0;32m      3\u001b[0m         \u001b[0mpipeline_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPELINE_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mpipeline_root\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPIPELINE_ROOT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mdata_root\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDATA_ROOT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tfx\\orchestration\\portable\\tfx_runner.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, pipeline, run_options, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m       \u001b[0mrun_options_pb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_with_ir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline_pb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_options_pb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tfx\\orchestration\\local\\local_dag_runner.py\u001b[0m in \u001b[0;36mrun_with_ir\u001b[1;34m(self, pipeline, run_options)\u001b[0m\n\u001b[0;32m    107\u001b[0m           \u001b[1;32mwith\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMetadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection_config\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmlmd_handle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mpartial_run_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnapshot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlmd_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0mcomponent_launcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Component %s is finished.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tfx\\orchestration\\portable\\launcher.py\u001b[0m in \u001b[0;36mlaunch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;31m# Runs as a normal node.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m     \u001b[0mexecution_preparation_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_execution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m     (execution_info, contexts,\n\u001b[0;32m    552\u001b[0m      \u001b[0mis_execution_needed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexecution_preparation_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecution_info\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tfx\\orchestration\\portable\\launcher.py\u001b[0m in \u001b[0;36m_prepare_execution\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    410\u001b[0m                   self._output_resolver.get_executor_output_uri(execution.id)),\n\u001b[0;32m    411\u001b[0m               stateful_working_dir=(\n\u001b[1;32m--> 412\u001b[1;33m                   self._output_resolver.get_stateful_working_directory()),\n\u001b[0m\u001b[0;32m    413\u001b[0m               tmp_dir=self._output_resolver.make_tmp_dir(execution.id)),\n\u001b[0;32m    414\u001b[0m           \u001b[0mexecution_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexecution\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tfx\\orchestration\\portable\\outputs_utils.py\u001b[0m in \u001b[0;36mget_stateful_working_directory\u001b[1;34m(self, execution_id)\u001b[0m\n\u001b[0;32m    221\u001b[0m                                         dir_suffix)\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m       \u001b[0mfileio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstateful_working_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m       logging.exception('Failed to make stateful working dir: %s',\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tfx\\dsl\\io\\fileio.py\u001b[0m in \u001b[0;36mmakedirs\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mPathType\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m   \u001b[1;34m\"\"\"Make a directory at the given path, recursively creating parents.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m   \u001b[0m_get_filesystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tfx\\dsl\\io\\plugins\\tensorflow_gfile.py\u001b[0m in \u001b[0;36mmakedirs\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mPathType\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mrecursive_create_dir_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m   \"\"\"\n\u001b[1;32m--> 511\u001b[1;33m   \u001b[0m_pywrap_file_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRecursivelyCreateDir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_to_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Failed to create a directory: pipelines\\penguin-simple\\CsvExampleGen\\.system\\stateful_working_dir/2022-08-27T18:32:54.408901; Invalid argument"
     ]
    }
   ],
   "source": [
    "tfx.orchestration.LocalDagRunner().run(\n",
    "    _create_pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        data_root=DATA_ROOT,\n",
    "        module_file=_trainer_module_file,\n",
    "        serving_model_dir=SERVING_MODEL_DIR,\n",
    "        metadata_path=METADATA_PATH\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
